{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j\n",
      "  Downloading neo4j-4.4.1.tar.gz (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 3.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from neo4j) (2021.1)\n",
      "Building wheels for collected packages: neo4j\n",
      "  Building wheel for neo4j (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neo4j: filename=neo4j-4.4.1-py3-none-any.whl size=114759 sha256=3481cf45e797f450dd94034b25ce4fcae5b92a00bf983e94cc28078dfd56357a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1a/38/4b/0876d24f853fdfe40b2440c8c03332ec2d7f1f88b2446dc694\n",
      "Successfully built neo4j\n",
      "Installing collected packages: neo4j\n",
      "Successfully installed neo4j-4.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import from_csv\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context\n",
    "def init():\n",
    "    spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('AndMalware-consumer')\n",
    "         # Add kafka package  \n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "         .getOrCreate())\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc = init()\n",
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "def getReadStream(spark):\n",
    "    df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"kafka-server:9092\") \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .option(\"subscribe\", \"AndMalwer\") \\\n",
    "      .load()\n",
    "    \n",
    "    #df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
    "    dff = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))\n",
    "    return dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Transform and write batchDF\n",
    "    df.write\\\n",
    "    .format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"neo\")\\\n",
    "    .option(\"query\", \"CREATE (n:Malware {DIP: event.Destination_IP,maxFD: event.MaxFD,minFD: event.MinFD,sumFD: event.SumFD,meanFD: event.MeanFD,stddevFD: event.StddevFD,maxTFWD: event.MaxTFWD,minTFWD: event.MinTFWD,sumTFWD: event.SumTFWD,meanTFWD: event.MeanTFWD,stddevTFWD: event.StddevTFWD,maxTBWD: event.MaxTBWD,minTBWD: event.MinTBWD,sumTBWD: event.SumTBWD,meanTBWD: event.MeanTBWD,stddevTBWD: event.StddevTBWD,maxFB: event.MaxFB,minFB: event.MinFB,sumFB: event.SumFB,meanFB: event.MeanFB,stddevFB: event.StddevFB,maxFP: event.MaxFB,minFP: event.MinFP,sumFP: event.SumFP,meanFP: event.MeanFP,stddevFP: event.StddevFP})\")\\\n",
    "    .save()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 2442, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 196, in call\n    raise e\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 193, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"<ipython-input-5-dda643ea7dcf>\", line 3, in foreach_batch_function\n    df.write\\\n  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1107, in save\n    self._jwrite.save()\n  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 111, in deco\n    return f(*a, **kw)\n  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o4143.save.\n: org.apache.spark.SparkException: Writing job aborted.\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:370)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 861.0 failed 1 times, most recent failure: Lost task 0.0 in stage 861.0 (TID 86431) (bd3602febbde executor driver): org.neo4j.driver.exceptions.ServiceUnavailableException: Unable to connect to neo4j:7687, ensure the database is running and that there is a working network connection to it.\n\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:143)\n\tat org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:98)\n\tat org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:92)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:55)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.commit(BaseDataWriter.scala:124)\n\tat org.neo4j.spark.writer.Neo4jDataWriter.commit(Neo4jDataWriter.scala:9)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:76)\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:70)\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:37)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\nCaused by: org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: neo4j/172.20.0.6:7687\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n\t... 32 more\nCaused by: org.neo4j.driver.exceptions.ServiceUnavailableException: Unable to connect to neo4j:7687, ensure the database is running and that there is a working network connection to it.\n\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:143)\n\tat org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:98)\n\tat org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:92)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:55)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.commit(BaseDataWriter.scala:124)\n\tat org.neo4j.spark.writer.Neo4jDataWriter.commit(Neo4jDataWriter.scala:9)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:76)\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:70)\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:37)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\nCaused by: org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: neo4j/172.20.0.6:7687\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\n=== Streaming Query ===\nIdentifier: [id = 230adad4-e408-4040-a692-45bc16fb42bf, runId = 5f106e52-76c5-491e-9030-83ff07ed26d0]\nCurrent Committed Offsets: {KafkaV2[Subscribe[AndMalwer]]: {\"AndMalwer\":{\"0\":1652}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[AndMalwer]]: {\"AndMalwer\":{\"0\":1654}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFilter (Destination_IP#88 =  10.42.0.151)\n+- Aggregate [window#348, Destination_IP#88], [window#348 AS window#205, Destination_IP#88, sum(cast(Flow_Duration#114 as bigint)) AS SumFD#219L, max(Flow_Duration#114) AS MaxFD#221, min(Flow_Duration#114) AS MinFD#223, avg(cast(Flow_Duration#114 as bigint)) AS MeanFD#225, stddev_samp(cast(Flow_Duration#114 as double)) AS StddevFD#235, sum(cast(Total_Fwd_Packets#127 as bigint)) AS SumTFWD#237L, max(Total_Fwd_Packets#127) AS MaxTFWD#239, min(Total_Fwd_Packets#127) AS MinTFWD#241, avg(cast(Total_Fwd_Packets#127 as bigint)) AS MeanTFWD#243, stddev_samp(cast(Total_Fwd_Packets#127 as double)) AS StddevTFWD#253, sum(cast(Total_Bwd_Packets#140 as bigint)) AS SumTBWD#255L, max(Total_Bwd_Packets#140) AS MaxTBWD#257, min(Total_Bwd_Packets#140) AS MinTBWD#259, avg(cast(Total_Bwd_Packets#140 as bigint)) AS MeanTBWD#261, stddev_samp(cast(Total_Bwd_Packets#140 as double)) AS StddevTBWD#271, sum(cast(Flow_Bytess#179 as bigint)) AS SumFB#273L, max(Flow_Bytess#179) AS MaxFB#275, min(Flow_Bytess#179) AS MinFB#277, avg(cast(Flow_Bytess#179 as bigint)) AS MeanFB#279, stddev_samp(cast(Flow_Bytess#179 as double)) AS StddevFB#289, sum(Flow_Packetss#192) AS SumFP#291, max(Flow_Packetss#192) AS MaxFP#293, ... 3 more fields]\n   +- Filter isnotnull(Timestamp#101)\n      +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) as double) = (cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) THEN (CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 600000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) as double) = (cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) THEN (CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 600000000) + 0) + 600000000), LongType, TimestampType)) AS window#348, Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#153, Total_Length_of_Bwd_Packets#166, Flow_Bytess#179, Flow_Packetss#192]\n         +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#153, Total_Length_of_Bwd_Packets#166, Flow_Bytess#179, cast(Flow_Packetss#48 as double) AS Flow_Packetss#192]\n            +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#153, Total_Length_of_Bwd_Packets#166, cast(Flow_Bytess#47 as int) AS Flow_Bytess#179, Flow_Packetss#48]\n               +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#153, cast(Total_Length_of_Bwd_Packets#46 as int) AS Total_Length_of_Bwd_Packets#166, Flow_Bytess#47, Flow_Packetss#48]\n                  +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, cast(Total_Length_of_Fwd_Packets#45 as int) AS Total_Length_of_Fwd_Packets#153, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                     +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, cast(Total_Bwd_Packets#44 as int) AS Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                        +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, cast(Total_Fwd_Packets#43 as int) AS Total_Fwd_Packets#127, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                           +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, cast(Flow_Duration#42 as int) AS Flow_Duration#114, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                              +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, cast(Timestamp#41 as timestamp) AS Timestamp#101, Flow_Duration#42, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                                 +- Project [Source_IP#37, Source_Port#61, Destination_IP#39 AS Destination_IP#88, Destination_Port#74, Timestamp#41, Flow_Duration#42, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                                    +- Project [Source_IP#37, Source_Port#61, Destination_IP#39, cast(Destination_Port#40 as int) AS Destination_Port#74, Timestamp#41, Flow_Duration#42, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                                       +- Project [Source_IP#37, cast(Source_Port#38 as int) AS Source_Port#61, Destination_IP#39, Destination_Port#40, Timestamp#41, Flow_Duration#42, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                                          +- Project [split(value#29, ,, -1)[0] AS Source_IP#37, split(value#29, ,, -1)[1] AS Source_Port#38, split(value#29, ,, -1)[2] AS Destination_IP#39, split(value#29, ,, -1)[3] AS Destination_Port#40, split(value#29, ,, -1)[4] AS Timestamp#41, split(value#29, ,, -1)[5] AS Flow_Duration#42, split(value#29, ,, -1)[6] AS Total_Fwd_Packets#43, split(value#29, ,, -1)[7] AS Total_Bwd_Packets#44, split(value#29, ,, -1)[8] AS Total_Length_of_Fwd_Packets#45, split(value#29, ,, -1)[9] AS Total_Length_of_Bwd_Packets#46, split(value#29, ,, -1)[10] AS Flow_Bytess#47, split(value#29, ,, -1)[11] AS Flow_Packetss#48]\n                                             +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                                                +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                                                   +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@d2890ea, KafkaV2[Subscribe[AndMalwer]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6ffa8aa7a982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m         .start())\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 2442, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 196, in call\n    raise e\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 193, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"<ipython-input-5-dda643ea7dcf>\", line 3, in foreach_batch_function\n    df.write\\\n  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1107, in save\n    self._jwrite.save()\n  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 111, in deco\n    return f(*a, **kw)\n  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o4143.save.\n: org.apache.spark.SparkException: Writing job aborted.\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:370)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 861.0 failed 1 times, most recent failure: Lost task 0.0 in stage 861.0 (TID 86431) (bd3602febbde executor driver): org.neo4j.driver.exceptions.ServiceUnavailableException: Unable to connect to neo4j:7687, ensure the database is running and that there is a working network connection to it.\n\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:143)\n\tat org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:98)\n\tat org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:92)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:55)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.commit(BaseDataWriter.scala:124)\n\tat org.neo4j.spark.writer.Neo4jDataWriter.commit(Neo4jDataWriter.scala:9)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:76)\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:70)\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:37)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\nCaused by: org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: neo4j/172.20.0.6:7687\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n\t... 32 more\nCaused by: org.neo4j.driver.exceptions.ServiceUnavailableException: Unable to connect to neo4j:7687, ensure the database is running and that there is a working network connection to it.\n\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:143)\n\tat org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:98)\n\tat org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:92)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:55)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:93)\n\tat org.neo4j.spark.writer.BaseDataWriter.commit(BaseDataWriter.scala:124)\n\tat org.neo4j.spark.writer.Neo4jDataWriter.commit(Neo4jDataWriter.scala:9)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:76)\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:70)\n\t\tat org.neo4j.driver.internal.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:37)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\nCaused by: org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: neo4j/172.20.0.6:7687\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\n=== Streaming Query ===\nIdentifier: [id = 230adad4-e408-4040-a692-45bc16fb42bf, runId = 5f106e52-76c5-491e-9030-83ff07ed26d0]\nCurrent Committed Offsets: {KafkaV2[Subscribe[AndMalwer]]: {\"AndMalwer\":{\"0\":1652}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[AndMalwer]]: {\"AndMalwer\":{\"0\":1654}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFilter (Destination_IP#88 =  10.42.0.151)\n+- Aggregate [window#348, Destination_IP#88], [window#348 AS window#205, Destination_IP#88, sum(cast(Flow_Duration#114 as bigint)) AS SumFD#219L, max(Flow_Duration#114) AS MaxFD#221, min(Flow_Duration#114) AS MinFD#223, avg(cast(Flow_Duration#114 as bigint)) AS MeanFD#225, stddev_samp(cast(Flow_Duration#114 as double)) AS StddevFD#235, sum(cast(Total_Fwd_Packets#127 as bigint)) AS SumTFWD#237L, max(Total_Fwd_Packets#127) AS MaxTFWD#239, min(Total_Fwd_Packets#127) AS MinTFWD#241, avg(cast(Total_Fwd_Packets#127 as bigint)) AS MeanTFWD#243, stddev_samp(cast(Total_Fwd_Packets#127 as double)) AS StddevTFWD#253, sum(cast(Total_Bwd_Packets#140 as bigint)) AS SumTBWD#255L, max(Total_Bwd_Packets#140) AS MaxTBWD#257, min(Total_Bwd_Packets#140) AS MinTBWD#259, avg(cast(Total_Bwd_Packets#140 as bigint)) AS MeanTBWD#261, stddev_samp(cast(Total_Bwd_Packets#140 as double)) AS StddevTBWD#271, sum(cast(Flow_Bytess#179 as bigint)) AS SumFB#273L, max(Flow_Bytess#179) AS MaxFB#275, min(Flow_Bytess#179) AS MinFB#277, avg(cast(Flow_Bytess#179 as bigint)) AS MeanFB#279, stddev_samp(cast(Flow_Bytess#179 as double)) AS StddevFB#289, sum(Flow_Packetss#192) AS SumFP#291, max(Flow_Packetss#192) AS MaxFP#293, ... 3 more fields]\n   +- Filter isnotnull(Timestamp#101)\n      +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) as double) = (cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) THEN (CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 600000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) as double) = (cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) THEN (CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(Timestamp#101, TimestampType, LongType) - 0) as double) / cast(600000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 600000000) + 0) + 600000000), LongType, TimestampType)) AS window#348, Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#153, Total_Length_of_Bwd_Packets#166, Flow_Bytess#179, Flow_Packetss#192]\n         +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#153, Total_Length_of_Bwd_Packets#166, Flow_Bytess#179, cast(Flow_Packetss#48 as double) AS Flow_Packetss#192]\n            +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#153, Total_Length_of_Bwd_Packets#166, cast(Flow_Bytess#47 as int) AS Flow_Bytess#179, Flow_Packetss#48]\n               +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#153, cast(Total_Length_of_Bwd_Packets#46 as int) AS Total_Length_of_Bwd_Packets#166, Flow_Bytess#47, Flow_Packetss#48]\n                  +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, Total_Bwd_Packets#140, cast(Total_Length_of_Fwd_Packets#45 as int) AS Total_Length_of_Fwd_Packets#153, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                     +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, Total_Fwd_Packets#127, cast(Total_Bwd_Packets#44 as int) AS Total_Bwd_Packets#140, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                        +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, Flow_Duration#114, cast(Total_Fwd_Packets#43 as int) AS Total_Fwd_Packets#127, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                           +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, Timestamp#101, cast(Flow_Duration#42 as int) AS Flow_Duration#114, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                              +- Project [Source_IP#37, Source_Port#61, Destination_IP#88, Destination_Port#74, cast(Timestamp#41 as timestamp) AS Timestamp#101, Flow_Duration#42, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                                 +- Project [Source_IP#37, Source_Port#61, Destination_IP#39 AS Destination_IP#88, Destination_Port#74, Timestamp#41, Flow_Duration#42, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                                    +- Project [Source_IP#37, Source_Port#61, Destination_IP#39, cast(Destination_Port#40 as int) AS Destination_Port#74, Timestamp#41, Flow_Duration#42, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                                       +- Project [Source_IP#37, cast(Source_Port#38 as int) AS Source_Port#61, Destination_IP#39, Destination_Port#40, Timestamp#41, Flow_Duration#42, Total_Fwd_Packets#43, Total_Bwd_Packets#44, Total_Length_of_Fwd_Packets#45, Total_Length_of_Bwd_Packets#46, Flow_Bytess#47, Flow_Packetss#48]\n                                          +- Project [split(value#29, ,, -1)[0] AS Source_IP#37, split(value#29, ,, -1)[1] AS Source_Port#38, split(value#29, ,, -1)[2] AS Destination_IP#39, split(value#29, ,, -1)[3] AS Destination_Port#40, split(value#29, ,, -1)[4] AS Timestamp#41, split(value#29, ,, -1)[5] AS Flow_Duration#42, split(value#29, ,, -1)[6] AS Total_Fwd_Packets#43, split(value#29, ,, -1)[7] AS Total_Bwd_Packets#44, split(value#29, ,, -1)[8] AS Total_Length_of_Fwd_Packets#45, split(value#29, ,, -1)[9] AS Total_Length_of_Bwd_Packets#46, split(value#29, ,, -1)[10] AS Flow_Bytess#47, split(value#29, ,, -1)[11] AS Flow_Packetss#48]\n                                             +- Project [key#21, cast(value#8 as string) AS value#29, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                                                +- Project [cast(key#7 as string) AS key#21, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n                                                   +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@d2890ea, KafkaV2[Subscribe[AndMalwer]]\n"
     ]
    }
   ],
   "source": [
    "#Geting readsteram\n",
    "df1 = getReadStream(sc)\n",
    "\n",
    "#Parsing datta from value string.\n",
    "\n",
    "df2 = df1.selectExpr(\"split(value,',')[0] as Source_IP\" \\\n",
    "                 ,\"split(value,',')[1] as Source_Port\" \\\n",
    "                 ,\"split(value,',')[2] as Destination_IP\" \\\n",
    "                ,\"split(value,',')[3] as Destination_Port\" \\\n",
    "                ,\"split(value,',')[4] as Timestamp\" \\\n",
    "                ,\"split(value,',')[5] as Flow_Duration\" \\\n",
    "                ,\"split(value,',')[6] as Total_Fwd_Packets\" \\\n",
    "                ,\"split(value,',')[7] as Total_Bwd_Packets\" \\\n",
    "                ,\"split(value,',')[8] as Total_Length_of_Fwd_Packets\" \\\n",
    "                ,\"split(value,',')[9] as Total_Length_of_Bwd_Packets\" \\\n",
    "                ,\"split(value,',')[10] as Flow_Bytess\" \\\n",
    "                ,\"split(value,',')[11] as Flow_Packetss\" \\\n",
    "                    )\n",
    "\n",
    "\n",
    "#Formating data.\n",
    "\n",
    "df3 = df2.withColumn(\"Source_Port\", df2[\"Source_Port\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"Destination_Port\", df2[\"Destination_Port\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"Destination_IP\", df2[\"Destination_IP\"].alias(\"DIP\"))\\\n",
    "        .withColumn(\"Timestamp\", df2[\"Timestamp\"].cast(TimestampType()))\\\n",
    "        .withColumn(\"Flow_Duration\", df2[\"Flow_Duration\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"Total_Fwd_Packets\", df2[\"Total_Fwd_Packets\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"Total_Bwd_Packets\", df2[\"Total_Bwd_Packets\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"Total_Length_of_Fwd_Packets\", df2[\"Total_Length_of_Fwd_Packets\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"Total_Length_of_Bwd_Packets\", df2[\"Total_Length_of_Bwd_Packets\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"Flow_Bytess\", df2[\"Flow_Bytess\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"Flow_Packetss\", df2[\"Flow_Packetss\"].cast(DoubleType()))\n",
    "\n",
    "#Creating a window duration 10 minutes.\n",
    "\n",
    "wind = window(df3.Timestamp,\"10 minutes\")\n",
    "\n",
    "wdf = df3.groupBy(wind,col(\"Destination_IP\")).agg(sum(col(\"Flow_Duration\")).alias(\"SumFD\")\\\n",
    "                                                         ,max(col(\"Flow_Duration\")).alias(\"MaxFD\")\\\n",
    "                                                         ,min(col(\"Flow_Duration\")).alias(\"MinFD\")\\\n",
    "                                                         ,mean(col(\"Flow_Duration\")).alias(\"MeanFD\")\\\n",
    "                                                         ,stddev(col(\"Flow_Duration\")).alias(\"StddevFD\")\\\n",
    "                                                         ,sum(col(\"Total_Fwd_Packets\")).alias(\"SumTFWD\")\\\n",
    "                                                         ,max(col(\"Total_Fwd_Packets\")).alias(\"MaxTFWD\")\\\n",
    "                                                         ,min(col(\"Total_Fwd_Packets\")).alias(\"MinTFWD\")\\\n",
    "                                                         ,mean(col(\"Total_Fwd_Packets\")).alias(\"MeanTFWD\")\\\n",
    "                                                         ,stddev(col(\"Total_Fwd_Packets\")).alias(\"StddevTFWD\")\n",
    "                                                         ,sum(col(\"Total_Bwd_Packets\")).alias(\"SumTBWD\")\\\n",
    "                                                         ,max(col(\"Total_Bwd_Packets\")).alias(\"MaxTBWD\")\\\n",
    "                                                         ,min(col(\"Total_Bwd_Packets\")).alias(\"MinTBWD\")\\\n",
    "                                                         ,mean(col(\"Total_Bwd_Packets\")).alias(\"MeanTBWD\")\\\n",
    "                                                         ,stddev(col(\"Total_Bwd_Packets\")).alias(\"StddevTBWD\")\n",
    "                                                         ,sum(col(\"Flow_Bytess\")).alias(\"SumFB\")\\\n",
    "                                                         ,max(col(\"Flow_Bytess\")).alias(\"MaxFB\")\\\n",
    "                                                         ,min(col(\"Flow_Bytess\")).alias(\"MinFB\")\\\n",
    "                                                         ,mean(col(\"Flow_Bytess\")).alias(\"MeanFB\")\\\n",
    "                                                         ,stddev(col(\"Flow_Bytess\")).alias(\"StddevFB\")\n",
    "                                                         ,sum(col(\"Flow_Packetss\")).alias(\"SumFP\")\\\n",
    "                                                         ,max(col(\"Flow_Packetss\")).alias(\"MaxFP\")\\\n",
    "                                                         ,min(col(\"Flow_Packetss\")).alias(\"MinFP\")\\\n",
    "                                                         ,mean(col(\"Flow_Packetss\")).alias(\"MeanFP\")\\\n",
    "                                                         ,stddev(col(\"Flow_Packetss\")).alias(\"StddevFP\"))\\\n",
    "                                                        .where(col(\"Destination_IP\") ==\" 10.42.0.151\")\n",
    "\n",
    "#Write stream.\n",
    "query = (wdf.writeStream\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .outputMode('update')\\\n",
    "        .trigger(processingTime='3 seconds')\\\n",
    "        .start())\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
