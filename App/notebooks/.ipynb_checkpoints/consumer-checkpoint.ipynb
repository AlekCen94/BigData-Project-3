{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import from_csv\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context\n",
    "def init():\n",
    "    spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('AndMalwareMLTest')\n",
    "         # Add kafka package  \n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "         .getOrCreate())\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a154027b-c241-4fe8-b261-fb0008609a5a;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.2/spark-sql-kafka-0-10_2.12-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2!spark-sql-kafka-0-10_2.12.jar (105ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.2/spark-token-provider-kafka-0-10_2.12-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2!spark-token-provider-kafka-0-10_2.12.jar (64ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.6.0/kafka-clients-2.6.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.6.0!kafka-clients.jar (1065ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (151ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.8-1/zstd-jni-1.4.8-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.8-1!zstd-jni.jar (3343ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (526ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (1627ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (60ms)\n",
      ":: resolution report :: resolve 5801ms :: artifacts dl 6982ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   9   |   9   |   0   ||   9   |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a154027b-c241-4fe8-b261-fb0008609a5a\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 0 already retrieved (13083kB/19ms)\n",
      "22/03/05 06:36:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = init()\n",
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "def getReadStream(spark):\n",
    "    df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"kafka-server:9092\") \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .option(\"subscribe\", \"MalModel\") \\\n",
    "      .load()\n",
    "    \n",
    "    #df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
    "    dff = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))\n",
    "    return dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_af2427526490, depth=5, numNodes=43, numClasses=2, numFeatures=5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "Model = DecisionTreeClassificationModel.load(\"hdfs://namenode:9000/user/jovyan/model\")\n",
    "print(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "\n",
    "    labelIndexer = StringIndexer(inputCol=\"Label\", outputCol=\"indexedLabel\").fit(df)\n",
    "    print(labelIndexer)\n",
    "    col = df.columns\n",
    "    del col[-1]\n",
    "    vecta = VectorAssembler(inputCols=col,outputCol=\"features\")\n",
    "    print(vecta)\n",
    "    dataTemp = vecta.transform(df)\n",
    "    #dataTemp = dataTemp.dropna()\n",
    "    featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(dataTemp)\n",
    "    dataTemp2 = featureIndexer.transform(dataTemp)\n",
    "    #print(featureIndexer)\n",
    "    predictions = Model.transform(dataTemp2)\n",
    "    predictions.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/05 06:42:27 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f3b07873-6e17-4c9e-911c-c3cd2acbf019. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/03/05 06:42:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexerModel: uid=StringIndexer_bda6ba4029d6, handleInvalid=error\n",
      "VectorAssembler_a03af1231b54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/05 06:42:28 ERROR MicroBatchExecution: Query [id = 790a6314-2eea-40f2-8cb7-e6a650839820, runId = f0f7c691-b836-4f69-b3f0-9160aa730106] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 581, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 196, in call\n",
      "    raise e\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 193, in call\n",
      "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
      "  File \"/tmp/ipykernel_51/511656058.py\", line 9, in foreach_batch_function\n",
      "    dataTemp = vecta.transform(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 217, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 350, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1309, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.IllegalArgumentException: Data type string of column FP is not supported.\n",
      "Data type string of column FB is not supported.\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy17.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 581, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 196, in call\n    raise e\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 193, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"/tmp/ipykernel_51/511656058.py\", line 9, in foreach_batch_function\n    dataTemp = vecta.transform(df)\n  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 217, in transform\n    return self._transform(dataset)\n  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 350, in _transform\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1309, in __call__\n    return_value = get_return_value(\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    raise converted from None\npyspark.sql.utils.IllegalArgumentException: Data type string of column FP is not supported.\nData type string of column FB is not supported.\n\n=== Streaming Query ===\nIdentifier: [id = 790a6314-2eea-40f2-8cb7-e6a650839820, runId = f0f7c691-b836-4f69-b3f0-9160aa730106]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[MalModel]]: {\"MalModel\":{\"0\":137}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [Flow_Duration#632, Flow_IAT_Mean#639, FP#622, FB#623, Total_Length_of_Fwd_Packets#646, cast(Label#625 as string) AS Label#653]\n+- Project [Flow_Duration#632, Flow_IAT_Mean#639, FP#622, FB#623, cast(Total_Length_of_Fwd_Packets#624 as double) AS Total_Length_of_Fwd_Packets#646, Label#625]\n   +- Project [Flow_Duration#632, cast(Flow_IAT_Mean#621 as double) AS Flow_IAT_Mean#639, FP#622, FB#623, Total_Length_of_Fwd_Packets#624, Label#625]\n      +- Project [cast(Flow_Duration#620 as double) AS Flow_Duration#632, Flow_IAT_Mean#621, FP#622, FB#623, Total_Length_of_Fwd_Packets#624, Label#625]\n         +- Project [split(value#612, ,, -1)[0] AS Flow_Duration#620, split(value#612, ,, -1)[1] AS Flow_IAT_Mean#621, split(value#612, ,, -1)[2] AS FP#622, split(value#612, ,, -1)[3] AS FB#623, split(value#612, ,, -1)[4] AS Total_Length_of_Fwd_Packets#624, split(value#612, ,, -1)[5] AS Label#625]\n            +- Project [key#604, cast(value#591 as string) AS value#612, topic#592, partition#593, offset#594L, timestamp#595, timestampType#596]\n               +- Project [cast(key#590 as string) AS key#604, value#591, topic#592, partition#593, offset#594L, timestamp#595, timestampType#596]\n                  +- StreamingDataSourceV2Relation [key#590, value#591, topic#592, partition#593, offset#594L, timestamp#595, timestampType#596], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@b1a4883, KafkaV2[Subscribe[MalModel]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51/1816482825.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         .start())\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 581, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 196, in call\n    raise e\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 193, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"/tmp/ipykernel_51/511656058.py\", line 9, in foreach_batch_function\n    dataTemp = vecta.transform(df)\n  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 217, in transform\n    return self._transform(dataset)\n  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 350, in _transform\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1309, in __call__\n    return_value = get_return_value(\n  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    raise converted from None\npyspark.sql.utils.IllegalArgumentException: Data type string of column FP is not supported.\nData type string of column FB is not supported.\n\n=== Streaming Query ===\nIdentifier: [id = 790a6314-2eea-40f2-8cb7-e6a650839820, runId = f0f7c691-b836-4f69-b3f0-9160aa730106]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[MalModel]]: {\"MalModel\":{\"0\":137}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [Flow_Duration#632, Flow_IAT_Mean#639, FP#622, FB#623, Total_Length_of_Fwd_Packets#646, cast(Label#625 as string) AS Label#653]\n+- Project [Flow_Duration#632, Flow_IAT_Mean#639, FP#622, FB#623, cast(Total_Length_of_Fwd_Packets#624 as double) AS Total_Length_of_Fwd_Packets#646, Label#625]\n   +- Project [Flow_Duration#632, cast(Flow_IAT_Mean#621 as double) AS Flow_IAT_Mean#639, FP#622, FB#623, Total_Length_of_Fwd_Packets#624, Label#625]\n      +- Project [cast(Flow_Duration#620 as double) AS Flow_Duration#632, Flow_IAT_Mean#621, FP#622, FB#623, Total_Length_of_Fwd_Packets#624, Label#625]\n         +- Project [split(value#612, ,, -1)[0] AS Flow_Duration#620, split(value#612, ,, -1)[1] AS Flow_IAT_Mean#621, split(value#612, ,, -1)[2] AS FP#622, split(value#612, ,, -1)[3] AS FB#623, split(value#612, ,, -1)[4] AS Total_Length_of_Fwd_Packets#624, split(value#612, ,, -1)[5] AS Label#625]\n            +- Project [key#604, cast(value#591 as string) AS value#612, topic#592, partition#593, offset#594L, timestamp#595, timestampType#596]\n               +- Project [cast(key#590 as string) AS key#604, value#591, topic#592, partition#593, offset#594L, timestamp#595, timestampType#596]\n                  +- StreamingDataSourceV2Relation [key#590, value#591, topic#592, partition#593, offset#594L, timestamp#595, timestampType#596], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@b1a4883, KafkaV2[Subscribe[MalModel]]\n"
     ]
    }
   ],
   "source": [
    "df1 = getReadStream(sc)\n",
    "\n",
    "df2 = df1.selectExpr(\"split(value,',')[0] as Flow_Duration\" \\\n",
    "                 ,\"split(value,',')[1] as Flow_IAT_Mean\" \\\n",
    "                 ,\"split(value,',')[2] as FP\" \\\n",
    "                ,\"split(value,',')[3] as FB\" \\\n",
    "                ,\"split(value,',')[4] as Total_Length_of_Fwd_Packets\" \\\n",
    "                ,\"split(value,',')[5] as Label\" \\\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "df3 = df2.withColumn(\"Flow_Duration\", df2[\"Flow_Duration\"].cast(DoubleType()))\\\n",
    "        .withColumn(\"Flow_IAT_Mean\", df2[\"Flow_IAT_Mean\"].cast(DoubleType()))\\\n",
    "        .withColumn(\"FP\", df2[\"FP\"].cast(DoubleType()))\\\n",
    "        .withColumn(\"FB\", df2[\"FB\"].cast(DoubleType()))\\\n",
    "        .withColumn(\"Total_Length_of_Fwd_Packets\", df2[\"Total_Length_of_Fwd_Packets\"].cast(DoubleType()))\\\n",
    "        .withColumn(\"Label\", df2[\"Label\"].cast(StringType()))\\\n",
    "\n",
    "\n",
    "\n",
    "query = (df3.writeStream\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .outputMode('update')\\\n",
    "        .trigger(processingTime='3 seconds')\\\n",
    "        .start())\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
